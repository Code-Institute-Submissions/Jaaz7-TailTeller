{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# Model Building and Training Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Answer Business requirement 2:\n",
        "The client requires a machine learning model developed to accurately identify dog breeds from images.\n",
        "* Implement callbacks for learning rate reduction and early stopping to enhance training efficiency.\n",
        "* Use the Adam optimizer for training the model.\n",
        "* Build a neural network model using dropout for regularization and a dense output layer for multi-class classification.\n",
        "\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* n_breeds.pkl\n",
        "* final_features.pkl\n",
        "* y.pkl\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* tailteller_model.keras\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-05-08 16:37:16.161536: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-05-08 16:37:16.161714: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-05-08 16:37:16.163406: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-05-08 16:37:16.186145: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-05-08 16:37:16.563396: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.models import save_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/home/jaaz/Desktop/project-5/TailTeller/jupyter_notebooks'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We want to make the parent of the current directory the new current directory.\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chdir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New current directory: /home/jaaz/Desktop/project-5/TailTeller\n"
          ]
        }
      ],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"New current directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Creating callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Callbacks are helper functions that a machine learning model can use during training to save model progress or stop training if nothing significant is happening."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up Learning Rate Annealer\n",
        "learning_rate_reduction = ReduceLROnPlateau(\n",
        "    monitor='val_acc', \n",
        "    factor=0.01, \n",
        "    patience=3, \n",
        "    min_lr=1e-5,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Prepare Callbacks for Early Stopping\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss', \n",
        "    patience=10, \n",
        "    restore_best_weights=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stablish hyperparameters.\n",
        "\n",
        "### Initialize an optimization algorithm used in training neural networks called Adam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "epochs = 50\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Initialize Adaptive Moment Estimation (Adam) optimizer\n",
        "adam_optimizer = Adam(\n",
        "    learning_rate=learning_rate, \n",
        "    beta_1=0.9, \n",
        "    beta_2=0.999, \n",
        "    epsilon=None, \n",
        "    amsgrad=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Building and Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have now the following:\n",
        "- Callbacks\n",
        "    * learning_rate_reduction\n",
        "    * early_stopping\n",
        "- Hyperparameteers:\n",
        "    * batch_size\n",
        "    * epochs\n",
        "    * learning_rate\n",
        "- Algorithm Optimizer:\n",
        "    * adam_optimizer\n",
        "\n",
        "We are now ready to build and train the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One important thing to notice: the 0.7 number is important in the following code.\n",
        "It's a parameter of \"Dropout\" layer in a neural network model. It specifies the dropout rate at which nodes (neurons) are randomly dropped during training.\n",
        "\n",
        "* 0.7 -> There is a 70% chance that the output of each neuron in the previous layer is set to zero during training.\n",
        "Meaning that only 30% of nodes contribute to the activation and backpropagation processes at each step.\n",
        "\n",
        "* Overfitting reduction: By dropping 70% of the neurons randomly, the network becomes less sensitive to the specific weights of neurons, this will reduce the risk of overfitting in the training data.\n",
        "\n",
        "* Model robustness: With such a high dropout rate, the remaining neurons take on the \"burden\" of adjusting to represent the missing neurons, potentially leading to a more robust representation of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jaaz/.conda/envs/py10/lib/python3.10/site-packages/keras/src/layers/regularization/dropout.py:42: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4205 - loss: 3.0102 - val_accuracy: 0.9198 - val_loss: 0.2584 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9396 - loss: 0.1933 - val_accuracy: 0.9238 - val_loss: 0.2235 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m 1/32\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9297 - loss: 0.1921"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jaaz/.conda/envs/py10/lib/python3.10/site-packages/keras/src/callbacks/callback_list.py:96: UserWarning: Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: accuracy,loss,val_accuracy,val_loss,learning_rate.\n",
            "  callback.on_epoch_end(epoch, logs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9540 - loss: 0.1544 - val_accuracy: 0.9355 - val_loss: 0.2241 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9675 - loss: 0.1151 - val_accuracy: 0.9159 - val_loss: 0.2451 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9737 - loss: 0.0945 - val_accuracy: 0.9267 - val_loss: 0.2287 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9778 - loss: 0.0803 - val_accuracy: 0.9326 - val_loss: 0.2247 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9867 - loss: 0.0560 - val_accuracy: 0.9316 - val_loss: 0.2249 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9854 - loss: 0.0558 - val_accuracy: 0.9326 - val_loss: 0.2221 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9892 - loss: 0.0425 - val_accuracy: 0.9267 - val_loss: 0.2405 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9892 - loss: 0.0412 - val_accuracy: 0.9247 - val_loss: 0.2342 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9907 - loss: 0.0347 - val_accuracy: 0.9257 - val_loss: 0.2341 - learning_rate: 0.0010\n",
            "Epoch 12/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9943 - loss: 0.0281 - val_accuracy: 0.9228 - val_loss: 0.2341 - learning_rate: 0.0010\n",
            "Epoch 13/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9921 - loss: 0.0295 - val_accuracy: 0.9257 - val_loss: 0.2404 - learning_rate: 0.0010\n",
            "Epoch 14/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9956 - loss: 0.0247 - val_accuracy: 0.9296 - val_loss: 0.2375 - learning_rate: 0.0010\n",
            "Epoch 15/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9961 - loss: 0.0226 - val_accuracy: 0.9277 - val_loss: 0.2353 - learning_rate: 0.0010\n",
            "Epoch 16/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9940 - loss: 0.0227 - val_accuracy: 0.9277 - val_loss: 0.2481 - learning_rate: 0.0010\n",
            "Epoch 17/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9966 - loss: 0.0191 - val_accuracy: 0.9286 - val_loss: 0.2384 - learning_rate: 0.0010\n",
            "Epoch 18/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9964 - loss: 0.0183 - val_accuracy: 0.9267 - val_loss: 0.2434 - learning_rate: 0.0010\n"
          ]
        }
      ],
      "source": [
        "with open('final_features.pkl', 'rb') as f:\n",
        "    final_features = pickle.load(f)\n",
        "\n",
        "with open('n_breeds.pkl', 'rb') as f:\n",
        "    n_breeds = pickle.load(f)\n",
        "\n",
        "with open('y.pkl', 'rb') as f:\n",
        "    y = pickle.load(f)\n",
        "\n",
        "# Initialize a Sequential Deep Learning model\n",
        "model = Sequential()\n",
        "\n",
        "# Add a Dropout layer to prevent overfitting with 70% dropout rate\n",
        "model.add(Dropout(0.7, input_shape=(final_features.shape[1],)))\n",
        "\n",
        "# Add a Dense output layer with softmax activation for multi-class classification\n",
        "model.add(Dense(n_breeds, activation='softmax'))\n",
        "\n",
        "# Compile the model with Adam optimizer and categorical crossentropy loss\n",
        "adam_optimizer = Adam()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=adam_optimizer,\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model with the feature data\n",
        "\n",
        "history = model.fit(\n",
        "    final_features, y,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[learning_rate_reduction, early_stopping]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "save_model(model, 'tailteller_model.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusions and Next steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model building and training was successfully performed.\n",
        "\n",
        "The model underwent significant training phases, demonstrating high accuracy, which suggests effective learning and generalization capabilities.\n",
        "\n",
        "## Next steps:\n",
        "\n",
        "- Model Testing\n",
        "\n",
        "    * Validation Testing: Perform additional testing using the test/ folder with dog images for cross-validation.\n",
        "    * Performance Metrics: Assess detailed performance metrics such as precision recall, F1-score and confusion matrix to identify any bias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
